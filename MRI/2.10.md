# Tokens and terms
*input*:"Friends,Roman and Countrymen"

*output*: Tokens
 - Friends
 - Romans
 - Countrymen

 A token is an instance of a sequence of characters

 Each such token is now candidate for an index entry, after further processing

 But what are valid tokens to emit?

 ## Issues in tokenization
  - genitivo sassone
  - nomi di persona
  - numeri
  - nelle varie lingue si hanno valori differenti

  bigramma insieme di due parole che ha senso solo insieme (es. San Francinsco, artificial Intelligence)

## Stop words
 - you exclude the commonest words
 - they have little semantic content (30%)
 - But the trend if away from doing this:
    - good compression
    - good query optimization
    - You need them for:
      - phrase queries
      - various song titles
      - relational queries

## Normalization terms
We need to "normalize" words in indexed text as well as query words into the same form

we define equivalence classes of terms

**Accents** <br>
**Umlauts**

query need to understand language to figure out the way to do the query


## Case Folding
Reduce all letters to lower case (there are some exceptions)

Insert in the index multiple form of the therm could be useful but less efficient


## Thesauri and soundex
Do we handle synonysm and homonysm ?
we have to rewrite equivalence classes 
or we expand queries

Spelling mistakes 


## Lemmatization
Reduce inflectional/variant forms to base form.

Lemmatization implies doing proper reduction to dictionary headword form.

## Stemming
Reduce terms to (proper) their “roots” before indexing

“Stemming” suggest crude affix chopping

Porter's algorithm (stemming for english)

# Scoring, Term weighting and vector space model

















