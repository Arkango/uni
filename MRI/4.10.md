## Ranked retrieval models
- Free Text queries 

## Query-document matching scores
Tenere conto dei sinonimi prima di calcolare uno score

# Jaccard coefficient
set A , set B

jaccard(A,B) = $| A \cap	 B | / | A \cup B |$

issue with jaccard for scoring: 
- term frequency
- rare terms 
- more sophisticated way of normalizing for lenght


## Bag of words model
insieme di parole, non viene tenuto conto dell'ordine
e data un'importanza alle singole occorrenze

## Term frequency tf
La rilevanza non cresce proporzionalmente con la frequenza del termine

**Log-frequency** weighting
w<sub>t,d</sub> { 1 + log<sub>10</sub>tf<sub>t,d</sub> if tf<sub>t,d</sub> > 0, 0 otherwise

score for a document-query pair: sum over terms t in both q and d:

score(d,q) = E <sub>t€qVd</sub>  (1 + log<sub>10</sub>tf<sub>t,d</sub> )


# Document frequency
- Rare terms in the whole collection are more informative the frequents terms

- es. arachnocentric

- a document containing this term is very likely to be relevant to the query
- we want a high weight for rare terms like arachnocentric

df number of documents which contains a specific term

idf weight
df<sub>t</sub> is the document frequency of t: the number of documents that contain t 

- is an inverse measure of the informativeness fo t

we define the idf (inverse document frequency)

$idf = \log (N/df_{t})$

domanda da quanto va idf da 1 a log(N)

idf has no effect on ranking one term queries - since there is one idf value for each term in a collection

# tf-idf weighting

$w_{t,d} = (1+\log (tf_{t,d})) \times \log(N/df_{t})$

# Final ranking
 
### $Score(d,q) = \Sigma_{t \in q \cap d}  tf_{t,d} \times idf_{t}$

Perchè lo score funziona ? 
Euristiche implementate tengono conte dei vari aspetti importanti nel processing dei documenti

# Passi che esegue il programma

1. cerchi documenti che contengono tutte le parole
2. esegui lo score per ogni documento
3. ordina i documenti

